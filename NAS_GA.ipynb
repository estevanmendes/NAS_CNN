{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 12:20:36.278079: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-21 12:20:36.640220: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 12:20:38.010420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from deap import base, creator,tools,algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 12:20:42.916680: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-11-21 12:20:42.916781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: estevanmendes-Inspiron-15-3511\n",
      "2023-11-21 12:20:42.916793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: estevanmendes-Inspiron-15-3511\n",
      "2023-11-21 12:20:42.916980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 470.223.2\n",
      "2023-11-21 12:20:42.917025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 470.223.2\n",
      "2023-11-21 12:20:42.917035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 470.223.2\n"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "class Dataframe2ImageDataset:\n",
    "    @staticmethod\n",
    "    def load_image(filepath):\n",
    "        raw = tf.io.read_file(filepath)        \n",
    "        tensor = tf.io.decode_image(raw)\n",
    "        tensor = tf.cast(tensor, tf.float32) / 255.0\n",
    "        return tensor\n",
    "\n",
    "    def __init__(self,df,path_column,label_column) -> None:\n",
    "        self.paths=df[path_column].values\n",
    "        self.labels=np.eye(2)[df[label_column].values]\n",
    "\n",
    "    def create_dataset(self):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((self.paths,self.labels))\n",
    "        dataset = dataset.map(lambda filepath, label: (self.load_image(filepath), label))\n",
    "        # self.dataset=dataset\n",
    "        return dataset\n",
    "    \n",
    "trainning_df=pd.read_csv('trainning_dataset.csv')\n",
    "validation_df=pd.read_csv('validation_dataset.csv')\n",
    "testing_df=pd.read_csv('testing_dataset.csv')\n",
    "\n",
    "# trainning_data=Dataframe2ImageDataset(trainning_df,'path','binary_label_code')\n",
    "# trainning_data.create_dataset()\n",
    "training_dataset=Dataframe2ImageDataset(trainning_df,'path','binary_label_code').create_dataset()\n",
    "validation_dataset=Dataframe2ImageDataset(validation_df,'path','binary_label_code').create_dataset()\n",
    "testing_df_dataset=Dataframe2ImageDataset(testing_df,'path','binary_label_code').create_dataset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths=15\n",
    "pool_of_features={1:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':64,'kernel_size':5,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  2:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':32,'kernel_size':5,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  3:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':16,'kernel_size':5,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  4:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':8,'kernel_size':5,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  5:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':64,'kernel_size':3,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  6:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':32,'kernel_size':3,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  7:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':16,'kernel_size':3,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  8:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':8,'kernel_size':3,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  9:{'layer':tf.keras.layers.Conv2D,\n",
    "                     'params':{'filters':8,'kernel_size':3,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  10:{'layer':tf.keras.layers.Conv2D,\n",
    "                      'params':{'filters':64,'kernel_size':1,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  11:{'layer':tf.keras.layers.Conv2D,\n",
    "                      'params':{'filters':32,'kernel_size':1,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  12:{'layer':tf.keras.layers.Conv2D,\n",
    "                      'params':{'filters':16,'kernel_size':1,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  13:{'layer':tf.keras.layers.Conv2D,\n",
    "                      'params':{'filters':8,'kernel_size':1,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  14:{'layer':tf.keras.layers.Conv2D,\n",
    "                      'params':{'filters':8,'kernel_size':1,'strides':1,'padding':'valid','activation':'relu'}},\n",
    "                  15:{'layer':tf.keras.layers.BatchNormalization,\n",
    "                      'params':{}},\n",
    "                  16:{'layer':tf.keras.layers.MaxPool2D,\n",
    "                      'params':{'pool_size':(2,2)}},\n",
    "                  17:{'layer':tf.keras.layers.MaxPool2D,\n",
    "                      'params':{'pool_size':(4,4)}},\n",
    "                  18:{'layer':tf.keras.layers.MaxPool2D,\n",
    "                      'params':{'pool_size':(6,6)}},\n",
    "                  19:{'layer':tf.keras.layers.MaxPool2D,\n",
    "                      'params':{'pool_size':(8,8)}},\n",
    "                  20:{'layer':tf.keras.layers.MaxPool2D,\n",
    "                      'params':{'pool_size':(10,10)}},\n",
    "                  21:{'layer':tf.keras.layers.AveragePooling2D,\n",
    "                      'params':{'pool_size':(2,2)}},\n",
    "                  22:{'layer':tf.keras.layers.AveragePooling2D,\n",
    "                      'params':{'pool_size':(4,4)}},\n",
    "                  23:{'layer':tf.keras.layers.AveragePooling2D,\n",
    "                      'params':{'pool_size':(6,6)}},\n",
    "                  24:{'layer':tf.keras.layers.AveragePooling2D,\n",
    "                      'params':{'pool_size':(10,10)}},\n",
    "                  25:{'layer':tf.keras.layers.GlobalMaxPool2D,\n",
    "                      'params':{}}, \n",
    "                  26:{'layer':tf.keras.layers.Dense,\n",
    "                      'params':{'units':100,'activation':'relu'}},\n",
    "                  27:{'layer':tf.keras.layers.Dense,\n",
    "                      'params':{'units':80,'activation':'relu'}},\n",
    "                  28:{'layer':tf.keras.layers.Dense,\n",
    "                      'params':{'units':60,'activation':'relu'}},\n",
    "                  29:{'layer':tf.keras.layers.Dense,\n",
    "                      'params':{'units':40,'activation':'relu'}}, \n",
    "                  30:{'layer':tf.keras.layers.Dense,\n",
    "                      'params':{'units':20,'activation':'relu'}},\n",
    "                  31:{'layer':tf.keras.layers.Dense,\n",
    "                      'params':{'units':10,'activation':'relu'}},\n",
    "                  32:{'layer':tf.keras.layers.Dense,\n",
    "                      'params':{'units':5,'activation':'relu'}},\n",
    "                  33:{'layer':None,\n",
    "                      'params':{}},\n",
    "                  34:{'layer':tf.keras.layers.Dropout,\n",
    "                      'params':{'rate':0.5}},\n",
    "                  35:{'layer':tf.keras.layers.Dropout,\n",
    "                      'params':{'rate':0.25}},\n",
    "                  36:{'layer':tf.keras.layers.Dropout,\n",
    "                      'params':{'rate':0.15}}\n",
    "                    \n",
    "                  }\n",
    "\n",
    "feaseable_paths={}\n",
    "\n",
    "\n",
    "pool_of_features_probability=np.array([3,3,3,3,3,3,3,3,3,3,3,3,3,3,20,3,3,3,3,3,3,3,3,9,3,3,3,3,3,3,3,3,20,2,2,2])\n",
    "pool_of_features_probability=pool_of_features_probability/pool_of_features_probability.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_flatten_need(model:tf.keras.Sequential,layer_to_be_add:tf.keras.layers,debug=False)->tf.keras.Sequential:\n",
    "    \n",
    "    \"\"\"\n",
    "        Checks if it is required to add a flatten layern, in order of connect dense layers into Convolutional and Maxpooling layers.\n",
    "    \"\"\"\n",
    "    assert 'dense' not in tf.keras.layers.BatchNormalization.__doc__.lower()[:30]\n",
    "    assert 'dense' not in tf.keras.layers.Conv2D.__doc__.lower()[:30]\n",
    "    assert 'dense' not in tf.keras.layers.MaxPooling2D.__doc__.lower()[:30]\n",
    "    assert 'dense' not in tf.keras.layers.GlobalAvgPool2D.__doc__.lower()[:30]\n",
    "    assert 'dense' in tf.keras.layers.Dense.__doc__.lower()[:30]\n",
    "\n",
    "    assert 'conv' not in tf.keras.layers.BatchNormalization.__doc__.lower()[:30]\n",
    "    assert 'conv' in tf.keras.layers.Conv2D.__doc__.lower()[:30]\n",
    "    assert 'conv' not in tf.keras.layers.MaxPooling2D.__doc__.lower()[:30]\n",
    "    assert 'conv' not in tf.keras.layers.GlobalAvgPool2D.__doc__.lower()[:30]\n",
    "    assert 'conv' not in tf.keras.layers.Dense.__doc__.lower()[:30]\n",
    "\n",
    "    assert 'pool' not in tf.keras.layers.BatchNormalization.__doc__.lower()[:30]\n",
    "    assert 'pool' not in tf.keras.layers.Conv2D.__doc__.lower()[:30]\n",
    "    assert 'pool' in tf.keras.layers.MaxPooling2D.__doc__.lower()[:30]\n",
    "    assert 'pool' in tf.keras.layers.GlobalAvgPool2D.__doc__.lower()[:30]\n",
    "    assert 'pool' not in tf.keras.layers.Dense.__doc__.lower()[:30]\n",
    "    \n",
    "    if debug:\n",
    "        print('layer to add :',layer_to_be_add)\n",
    "    layers=model.layers\n",
    "    if len(layers)>1:\n",
    "        if 'dense' in layer_to_be_add.__doc__.lower()[:30]:\n",
    "            for previus_layer in np.flip(layers):\n",
    "                if 'dense' in previus_layer.__doc__.lower()[:30] or 'flat' in previus_layer.__doc__.lower()[:30] :\n",
    "                    break\n",
    "                elif ('conv' in previus_layer.__doc__.lower()[:30] or 'pool' in previus_layer.__doc__.lower()[:30]):\n",
    "                    model.add(tf.keras.layers.Flatten())\n",
    "                    break\n",
    "    return model\n",
    "\n",
    "def architecture_feaseable(individual,debug=False):\n",
    "    \"\"\"\n",
    "    creates the model indicated by the individual. \n",
    "    \"\"\"\n",
    "    model=tf.keras.Sequential()\n",
    "    non_empty_layer=0\n",
    "    for (index,gene) in enumerate(individual):\n",
    "        layer_details=pool_of_features[gene]      \n",
    "        \n",
    "              \n",
    "        if layer_details['layer'] is not None:\n",
    "            \n",
    "            if non_empty_layer==0:\n",
    "                layer_details['params']['input_shape']=(100,100,3)\n",
    "                layer=layer_details['layer'](**layer_details['params'])\n",
    "            else:\n",
    "                model=check_flatten_need(model,layer,debug=debug)\n",
    "                layer=layer_details['layer'](**layer_details['params'])\n",
    "            \n",
    "            try:            \n",
    "                model.add(layer)\n",
    "            except ValueError:\n",
    "                model=None\n",
    "                break\n",
    "\n",
    "    if model is None:\n",
    "        return [-1]*len(individual)\n",
    "    else:\n",
    "        return individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool_individuals=np.random.choice(list(pool_of_features.keys()),size=(1000,max_depths),p=pool_of_features_probability)\n",
    "# pool_individuals_valids=[]\n",
    "# for ind in pool_individuals:   \n",
    "#     pool_individuals_valids.append(architecture_feaseable(ind))\n",
    "\n",
    "# pool_individuals_valids=np.array(pool_individuals_valids)\n",
    "# pool_individuals_valids=pool_individuals_valids[np.where(pool_individuals_valids.sum(axis=1)>0)[0]]\n",
    "\n",
    "# with open('arquiteturas_validas.json','+w') as f:\n",
    "#     json.dump(pool_individuals_valids.tolist(),f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function and Enconding (f:individual -> Neural Network )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_checked={}\n",
    "\n",
    "def get_random_layer()->tf.keras.layers:\n",
    "    \"\"\" selects one random layer from the pool of features\"\"\"\n",
    "    layer_index=np.random.choice(list(pool_of_features.keys()),1,p=pool_of_features_probability)[0]\n",
    "    layer_details=pool_of_features[layer_index]\n",
    "    if layer_details['layer'] is not None:\n",
    "        layer=layer_details['layer'](**layer_details['params'])\n",
    "    else:\n",
    "        layer=get_random_layer()\n",
    "        \n",
    "    \n",
    "    return layer\n",
    "\n",
    "def check_dimension_compatibility(model:tf.keras.Sequential,layer:tf.keras.layers,debug=False) -> tf.keras.layers:\n",
    "    \"\"\"\n",
    "    checks if it is feasible to add the intended layer \n",
    "    \"\"\"\n",
    "    try:\n",
    "        ### dumb way of check compatibilty\n",
    "        model.add(layer)\n",
    "        model.pop()\n",
    "        \n",
    "    except ValueError:\n",
    "        if debug:\n",
    "            print('Dimension compatibility error')\n",
    "\n",
    "        layer=get_random_layer()\n",
    "        layer=check_dimension_compatibility(model,layer)\n",
    "\n",
    "    if debug:\n",
    "        print('dimension outcome:',layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "def check_flatten_need(model:tf.keras.Sequential,layer_to_be_add:tf.keras.layers,debug=False)->tf.keras.Sequential:\n",
    "    \n",
    "    \"\"\"\n",
    "        Checks if it is required to add a flatten layern, in order of connect dense layers into Convolutional and Maxpooling layers.\n",
    "    \"\"\"\n",
    "    assert 'dense' not in tf.keras.layers.BatchNormalization.__doc__.lower()[:30]\n",
    "    assert 'dense' not in tf.keras.layers.Conv2D.__doc__.lower()[:30]\n",
    "    assert 'dense' not in tf.keras.layers.MaxPooling2D.__doc__.lower()[:30]\n",
    "    assert 'dense' not in tf.keras.layers.GlobalAvgPool2D.__doc__.lower()[:30]\n",
    "    assert 'dense' in tf.keras.layers.Dense.__doc__.lower()[:30]\n",
    "\n",
    "    assert 'conv' not in tf.keras.layers.BatchNormalization.__doc__.lower()[:30]\n",
    "    assert 'conv' in tf.keras.layers.Conv2D.__doc__.lower()[:30]\n",
    "    assert 'conv' not in tf.keras.layers.MaxPooling2D.__doc__.lower()[:30]\n",
    "    assert 'conv' not in tf.keras.layers.GlobalAvgPool2D.__doc__.lower()[:30]\n",
    "    assert 'conv' not in tf.keras.layers.Dense.__doc__.lower()[:30]\n",
    "\n",
    "    assert 'pool' not in tf.keras.layers.BatchNormalization.__doc__.lower()[:30]\n",
    "    assert 'pool' not in tf.keras.layers.Conv2D.__doc__.lower()[:30]\n",
    "    assert 'pool' in tf.keras.layers.MaxPooling2D.__doc__.lower()[:30]\n",
    "    assert 'pool' in tf.keras.layers.GlobalAvgPool2D.__doc__.lower()[:30]\n",
    "    assert 'pool' not in tf.keras.layers.Dense.__doc__.lower()[:30]\n",
    "    \n",
    "    if debug:\n",
    "        print('layer to add :',layer_to_be_add)\n",
    "    layers=model.layers\n",
    "    if len(layers)>1:\n",
    "        if 'dense' in layer_to_be_add.__doc__.lower()[:30]:\n",
    "            for previus_layer in np.flip(layers):\n",
    "                if 'dense' in previus_layer.__doc__.lower()[:30] or 'flat' in previus_layer.__doc__.lower()[:30] :\n",
    "                    break\n",
    "                elif ('conv' in previus_layer.__doc__.lower()[:30] or 'pool' in previus_layer.__doc__.lower()[:30]):\n",
    "                    model.add(tf.keras.layers.Flatten())\n",
    "                    break\n",
    "    return model\n",
    "\n",
    "def create_model(individual,debug=False):\n",
    "    \"\"\"\n",
    "    creates the model indicated by the individual. \n",
    "    \"\"\"\n",
    "    model=tf.keras.Sequential()\n",
    "    non_empty_layer=0\n",
    "    for (index,gene) in enumerate(individual):\n",
    "        layer_details=pool_of_features[gene]      \n",
    "                      \n",
    "        if layer_details['layer'] is not None:\n",
    "            \n",
    "            if non_empty_layer==0:\n",
    "                layer_details['params']['input_shape']=(100,100,3)\n",
    "                layer=layer_details['layer'](**layer_details['params'])\n",
    "            else:\n",
    "                layer=layer_details['layer'](**layer_details['params'])\n",
    "                model=check_flatten_need(model,layer,debug=debug)\n",
    "                layer=layer_details['layer'](**layer_details['params'])\n",
    "                layer=check_dimension_compatibility(model,layer,debug=debug)\n",
    "            \n",
    "            model.add(layer)\n",
    "            non_empty_layer+=1   \n",
    "\n",
    "            \n",
    "    layer=tf.keras.layers.Dense(2,activation='softmax')\n",
    "    model=check_flatten_need(model,layer)\n",
    "    model.add(layer)\n",
    "    if debug:\n",
    "            print('model stack:',*model.layers,sep='\\n')\n",
    "\n",
    "    learning_rate=tf.optimizers.schedules.ExponentialDecay(initial_learning_rate=.1,decay_steps=10000.,decay_rate=0.95)\n",
    "    opt=tf.optimizers.SGD(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss=tf.metrics.mse,metrics=tf.metrics.AUC(name='auc'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model:tf.keras.Sequential,individual,verbose=0)-> tf.keras.Sequential:\n",
    "    if str(individual) not in space_checked.keys():\n",
    "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=3)\n",
    "        model.fit(training_dataset.batch(10),validation_data=validation_dataset.batch(10),epochs=20,verbose=verbose,callbacks=[callback,tensorboard_callback])   \n",
    "        space_checked[str(individual)]=model\n",
    "    else:\n",
    "        model=space_checked[str(individual)]\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model:tf.keras.Sequential,verbose=0)->float:\n",
    "    _,metric=model.evaluate(testing_df_dataset.batch(32),verbose=verbose)\n",
    "    return metric\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(individual,num_of_evaluations=1,verbose=0):\n",
    "    seeds=[1234,345,121,132,234]\n",
    "    metrics=[]\n",
    "    for seed in seeds[:num_of_evaluations]:\n",
    "        model=create_model(individual)\n",
    "        model=train_model(model,individual,verbose=verbose)\n",
    "        metrics.append(evaluate_model(model,verbose=verbose))\n",
    "    metrics=np.mean(metrics)\n",
    "    \n",
    "    return metrics,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing=False\n",
    "if testing:\n",
    "    individual=[12,#conv\n",
    "                16,#pooling\n",
    "                15,#norm\n",
    "                33,#empty\n",
    "                34,#dropout\n",
    "                1,#conv\n",
    "                2,#conv\n",
    "                3,#conv\n",
    "                33#empty\n",
    "                ]\n",
    "    model=create_model(individual,debug=True)\n",
    "    model=train_model(model,individual,verbose=1)\n",
    "    evaluate_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing:\n",
    "    for individual in np.random.choice(list(pool_of_features.keys()),size=(100,max_depths),p=pool_of_features_probability):\n",
    "        \n",
    "        model=create_model(individual)\n",
    "        print(individual,model.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice(a,p):\n",
    "   return np.random.choice(a=a,size=1,p=p)[0]\n",
    "\n",
    "\n",
    "def initIndividual(icls, content):\n",
    "    return icls(content)\n",
    "\n",
    "def initPopulation(pcls, ind_init,pop_size,trial_name, filename):\n",
    "    with open(filename, \"r\") as pop_file:\n",
    "        contents = np.array(json.load(pop_file))\n",
    "    # contents=np.array(contents)\n",
    "    index_ind_selected=np.random.choice(np.arange(0,len(contents)),size=pop_size,replace=False)\n",
    "    pop=contents[index_ind_selected,:]\n",
    "\n",
    "    with open(trial_name+'_population_selected.json','w') as f:\n",
    "        json.dump(pop.tolist(),f)\n",
    "\n",
    "\n",
    "    return pcls(ind_init(c) for c in pop)\n",
    "\n",
    "history = tools.History()\n",
    "\n",
    "\n",
    "\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "toolbox = base.Toolbox()\n",
    "# toolbox.register(\"attribute\", choice,a=list(pool_of_features.keys()),p=pool_of_features_probability)\n",
    "# toolbox.register(\"individual\", tools.initRepeat, creator.Individual,toolbox.attribute, n=15)\n",
    "# toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"individual_guess\", initIndividual, creator.Individual)\n",
    "toolbox.register(\"population_guess\", initPopulation, list, toolbox.individual_guess, filename=\"arquiteturas_validas.json\",trial_name='001')\n",
    "\n",
    "toolbox.register(\"mate\", tools.cxOnePoint)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt,low=0,up=len(pool_of_features), indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "toolbox.register(\"evaluate\", evaluate,num_of_evaluations=1,verbose=0)\n",
    "\n",
    "# Decorate the variation operators\n",
    "toolbox.decorate(\"mate\", history.decorator)\n",
    "toolbox.decorate(\"mutate\", history.decorator)\n",
    "\n",
    "\n",
    "population_size=5\n",
    "# pop=toolbox.population(n=population_size)\n",
    "population = toolbox.population_guess(pop_size=population_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hof = tools.HallOfFame(1)  # salva o melhor individuo que já existiu na pop durante a evolução\n",
    "\n",
    "# Gerar as estatísticas\n",
    "stats = tools.Statistics(lambda ind:ind.fitness.values)\n",
    "stats.register('avg', np.mean)\n",
    "stats.register('std', np.std)\n",
    "stats.register('min', np.min)\n",
    "stats.register('max', np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "generations=30\n",
    "pop, log = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.01, ngen=generations, stats=stats, halloffame=hof, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = networkx.DiGraph(history.genealogy_tree)\n",
    "graph = graph.reverse()     # Make the graph top-down\n",
    "colors = [toolbox.evaluate(history.genealogy_history[i])[0] for i in graph]\n",
    "networkx.draw(graph, node_color=colors)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hof[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model(hof[0]).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(hof[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(f'experiment_gen_{generations}_pop_{population_size}_{datetime.datetime.now()}.json','w+') as f:\n",
    "  json.dump(log,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
